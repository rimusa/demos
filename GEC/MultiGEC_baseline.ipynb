{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiGEC Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook was put together by Ricardo ([@rimusa](https://github.com/rimusa) on GitHub) and is based on the code found [here](https://github.com/spraakbanken/multigec-2025/blob/main/scripts/baseline.py)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to showcase the baseline for the [MultiGEC shared task](https://github.com/spraakbanken/multigec-2025) on grammatical error correction.\n",
    "If you want to learn a bit more about this task, be sure to check out the shared task homepage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is a minimal one-shot prompt-engineering baseline.\n",
    "This is a lot of buzzwords, let's see what they mean:\n",
    "\n",
    "* A _baseline_ is a system with which we're comparing ours with. There are two (often opposed) philosophies here:\n",
    "    * It should be simple to avoid introducing noise/artifacts to the task\n",
    "    * It should be well-performing to allow for better comparison of the results\n",
    "    \n",
    "* A _minimal baseline_ follows the \"being simple\" approach, trying to avoid any kind of over-engineering\n",
    "\n",
    "* A _few-shot system_ is meant to do very little adaptation (if at all) to the task in hand. It can be either finetuning with few examples for a small amount of epochs (with transformer-based models I'd say one at most) or showing the examples as part of a prompt.\n",
    "    * _Zero-shot systems_ do not get any examples at all, they are meant to test the model as-is.\n",
    "    * _One-shot systems_ see only one example. The idea here is that we don't want the model to change its internal representations but we do want it to adapt its output to the format of the input.\n",
    "\n",
    "* _Prompt-engineering_ is a method of getting generative language models to adapt to our task without actually changing the internal representations of the model. This way we can bootstrap the knowledge contained within without risking it forgetting things during finetuning (also known as \"catastrophic forgetting\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section has all of the imports needed to run this code.\n",
    "\n",
    "Note that to be able to run this notebook you need to use a Python 3 environment that has the following packages installed:\n",
    "\n",
    "```python\n",
    "# General Imports\n",
    "jupyter             # For reasons that hopefully are obvious\n",
    "\n",
    "# HuggingFace Imports\n",
    "transformers        # To be able to load the models\n",
    "huggingface_hub     # Llama is a gated model, so you need to log in\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good etiquette says to use only one GPU from the MLT server.\n",
    "Thus we will use just the one and a smaller model.\n",
    "For the 8B parameter model that we used for the shared task, we might need to use more GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"auto\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that we want two different kinds of prompts: one for doing minimal edits to the text and another one that allows for fluency edits.\n",
    "Thus, we have two different prompts.\n",
    "Note that the prompt has parts in common between the two versions, so we only need to define those parts once to reduce the amount of duplication (and thus diminishing the risk that the two diverge from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the shared part of the prompt\n",
    "task_prompt = \"You are a grammatical error correction tool. Your task is to correct the grammaticality and spelling of the input essay written by a learner of {}. {} Return only the corrected text and nothing more.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are two sets of brackets.\n",
    "We will put the language of the text to be corrected in the first set of brackets and the task description in the second one.\n",
    "\n",
    "We can then define the specific variations of the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_descriptions = {\n",
    "\n",
    "    \"minimal\" : \"Make the smallest possible change in order to make the essay grammatically correct. Change as few words as possible. Do not rephrase parts of the essay that are already grammatical. Do not change the meaning of the essay by adding or removing information. If the essay is already grammatically correct, you should output the original essay without changing anything.\",\n",
    "\n",
    "    \"fluency\" : \"You may rephrase parts of the essay to improve fluency. Do not change the meaning of the essay by adding or removing information. If the essay is already grammatically correct and fluent, you should output the original essay without changing anything.\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the two different versions of the prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt for when we use minimal edits:\n",
      "\n",
      "You are a grammatical error correction tool. Your task is to correct the grammaticality and spelling of the input essay written by a learner of LANG. Make the smallest possible change in order to make the essay grammatically correct. Change as few words as possible. Do not rephrase parts of the essay that are already grammatical. Do not change the meaning of the essay by adding or removing information. If the essay is already grammatically correct, you should output the original essay without changing anything. Return only the corrected text and nothing more.\n",
      "\n",
      "\n",
      "Prompt for when we use fluency edits:\n",
      "\n",
      "You are a grammatical error correction tool. Your task is to correct the grammaticality and spelling of the input essay written by a learner of LANG. You may rephrase parts of the essay to improve fluency. Do not change the meaning of the essay by adding or removing information. If the essay is already grammatically correct and fluent, you should output the original essay without changing anything. Return only the corrected text and nothing more.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task, description in task_descriptions.items():\n",
    "    print(\"\")\n",
    "    print(\"Prompt for when we use\", task, \"edits:\")\n",
    "    print(\"\")\n",
    "    print(task_prompt.format(\"LANG\",description))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readying the Prompts for Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama requires a specific format for the prompts.\n",
    "I will not go into any details here, but I can point you [in the right direction](https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/) in case you'd like to know more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the one-shot prompt for Llama 3\n",
    "one_shot =  \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{}\n",
    "\n",
    "Input essay:\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{}<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Output essay:\n",
    "{}<|eot_id|>\n",
    "\n",
    "Input essay:\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Output essay:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be how the zero-shot version looks like\n",
    "# Note that it has one less turn in the conversation\n",
    "zero_shot =  \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{}\n",
    "\n",
    "Input essay:\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{}<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Output essay:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\n",
    "    \"one_shot\"  : one_shot,\n",
    "    \"zero_shot\" : zero_shot,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our prompt is one-shot, so we need to give it an example.\n",
    "The more nuanced the example is and the more similar it is to the actual input, the better the prompt will be.\n",
    "However, our baseline is supposed to be simple, so we only care that the model gets the in/out format correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_in  = \"My name is Susanna. I come from Berlin, in the middl of Germany, bot I live in Bungaborg. I am studying data science in the University of Bungaborg and work extra as a teacher.\"\n",
    "\n",
    "example_out = \"My name is Susanna. I come from Berlin, in the middle of Germany, but I live in Bungaborg. I am studying data science in the University of Bungaborg and work extra as a teacher.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the shared task baseline we used Llama 3.1 as it is one of the better-performing models as of the end of 2024.\n",
    "Note that you need to have a HuggingFace account and sell your soul to Facebook in order to use it.\n",
    "You can do so [here](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ricardomunozsanchez/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# First we log into HuggingFace, for this you need to use an authorization key, which is private.\n",
    "# Do not show or share your key with anyone, under any circumstances.\n",
    "\n",
    "# We save our key in a file called hf_key.txt\n",
    "with open(\"./hf_key.txt\") as F:\n",
    "    login(next(F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d048a357274d3f915f5e861df1422c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can then load the model\n",
    "\n",
    "checkpoint = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint, max_new_tokens=3000)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                            checkpoint,\n",
    "                                            config=config,\n",
    "                                            torch_dtype=\"auto\",\n",
    "                                            device_map=device\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse code for when getting the prompt. This will allow us to iterate in a faster way when giving examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_prompt(system, task, essay, language):\n",
    "\n",
    "    current_task = task_descriptions[task]\n",
    "\n",
    "    task_prompt.format(language,current_task)\n",
    "\n",
    "    items = [task_prompt]\n",
    "    if \"one\" in system:\n",
    "        items += [example_in, example_out]\n",
    "    items += [essay]\n",
    "    \n",
    "    current_prompt = prompts[system].format(*items)\n",
    "    \n",
    "    return current_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reuse the same code when obtaining predictions from the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(system, task, essay, language, max_length=300, print_output=True):\n",
    "\n",
    "    prompt = fetch_prompt(system, task, essay, language)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    out_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    correction = out_text.split(\"Output essay:\\n\")[-1].strip(\"<|eot_id|>\")\n",
    "\n",
    "    if print_output:\n",
    "        print(correction)\n",
    "    else:\n",
    "        return correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test our model with a simple essay here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language of the essay\n",
    "language = \"English\"\n",
    "\n",
    "# Text of the essay\n",
    "# NOTE - If the essay is too long, you'll get an error from the model!\n",
    "essay = \"\"\"This is a test esay.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The One-Shot Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the minimal edit prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test essay.\n"
     ]
    }
   ],
   "source": [
    "predict(\"one_shot\",\"minimal\",essay,language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the fluency edits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test essay.\n"
     ]
    }
   ],
   "source": [
    "predict(\"one_shot\",\"fluency\",essay,language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Zero-Shot Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tends to spit more gibberish than the one-shot example, but should still work most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimal edit prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test essay.\n"
     ]
    }
   ],
   "source": [
    "predict(\"zero_shot\",\"minimal\",essay,language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLuency edit prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test essay.\n"
     ]
    }
   ],
   "source": [
    "predict(\"zero_shot\",\"fluency\",essay,language)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multigec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
